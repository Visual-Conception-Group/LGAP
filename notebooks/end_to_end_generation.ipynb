{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52642362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"make variations of input image\"\"\"\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "sys.path.append(os.path.abspath(\"/raid/home/himanshus/experiments/repos/stable-diffusion/blip\"))\n",
    "\n",
    "import argparse, os, sys, glob\n",
    "import PIL\n",
    "import torch\n",
    "\n",
    "# original_backward = torch.Tensor.backward\n",
    "\n",
    "# def patched_backward(tensor, *args, **kwargs):\n",
    "#     kwargs['allow_unused'] = True\n",
    "#     original_backward(tensor, *args, **kwargs)\n",
    "\n",
    "# torch.Tensor.backward = patched_backward\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torch import autocast\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from contextlib import nullcontext\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "import robustbench\n",
    "from robustbench.data import load_cifar10, load_imagenet\n",
    "from robustbench.utils import load_model, clean_accuracy\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from scripts.custom_Dataset import CustomDataset\n",
    "from IPython.display import display, clear_output\n",
    "from custom_pgd import PGD\n",
    "from blip.models.blip import blip_decoder\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision.transforms.functional import InterpolationMode\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import foolbox as fb\n",
    "# from robustbench.data import load_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756cf60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parser():\n",
    "    def __init__(self):\n",
    "        self.prompt = \"a painting of a virus monster playing guitar\"\n",
    "        self.init_img = \"outputs/img2img-samples/grid-0003.png\" #path to the input image\n",
    "        self.outdir = \"../BLIP/experiment_1/imagenet/clean_test_generated_2048\" #dir to write results to\n",
    "        self.skip_grid = True\n",
    "        self.skip_save = False\n",
    "        self.ddim_steps = 50 #number of ddim sampling steps\n",
    "        self.plms = False\n",
    "        self.fixed_code = False\n",
    "        self.ddim_eta = 0.0 #ddim eta (eta=0.0 corresponds to deterministic sampling\n",
    "        self.n_iter = 1\n",
    "        self.C = 4 #latent channels\n",
    "        self.f = 8\n",
    "        self.n_samples = 32 #batch size\n",
    "        self.n_rows = 0\n",
    "        self.scale = 5.0 #unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "        self.strength = 0.5 #strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
    "        self.from_file = \"\"\n",
    "        self.config = \"configs/stable-diffusion/v1-inference.yaml\"\n",
    "        self.ckpt = \"models/ldm/stable-diffusion-v1/model.ckpt\"\n",
    "        self.seed = 42\n",
    "        self.precision = \"autocast\" #[\"full\", \"autocast\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960e1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_img(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e156098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape position embedding from 196 to 576\n",
      "load checkpoint from ./blip/checkpoints/model_base_capfilt_large.pth\n"
     ]
    }
   ],
   "source": [
    "from blip.models.blip import blip_decoder\n",
    "\n",
    "image_size = 384\n",
    "# image = load_demo_image(image_size=image_size, device=device)\n",
    "\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
    "model_path = \"./blip/checkpoints/model_base_capfilt_large.pth\"\n",
    "    \n",
    "blip_model = blip_decoder(pretrained=model_path, image_size=image_size, vit='base')\n",
    "blip_model.eval()\n",
    "blip_model = blip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ce6ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/ldm/stable-diffusion-v1/model.ckpt\n",
      "Global Step: 840000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'text_projection.weight', 'logit_scale', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "opt = parser()\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e60457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#added by himanshu for adding batch processing\n",
    "blip_normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "classifier_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "classifier_transform = transforms.Compose([transforms.ToTensor()])\n",
    "                                # transforms.Resize((96,96)),])\n",
    "                                # classifier_normalize])\n",
    "transform_blip = transforms.Compose([transforms.Resize(size=(384,384)),\n",
    "                                blip_normalize\n",
    "                                ])\n",
    "classifier_inv_normalize = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ]),\n",
    "                               ])\n",
    "blip_inv_normalize = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1./0.26862954, 1./0.26130258, 1./0.27577711 ]),\n",
    "                                transforms.Normalize(mean = [ -0.48145466, -0.4578275, -0.40821073 ],\n",
    "                                                     std = [ 1., 1., 1. ]),])\n",
    "                                # transforms.Resize((32,32))\n",
    "                            #    ])\n",
    "inverse_resize = transforms.Compose([transforms.Resize((32,32))])\n",
    "# inverse_resize_2 = transforms.Compose([transforms.Resize((32,32))])\n",
    "batch_size = opt.n_samples\n",
    "num_classes = 100\n",
    "# image_folder = '../BLIP/experiment_1/cifar10/clean_test/'\n",
    "# label_file = '../BLIP/experiment_1/cifar10/clean_test_labels.txt'\n",
    "image_folder = '../BLIP/experiment_1/imagenet/clean_test_2048'\n",
    "label_file = '../BLIP/experiment_1/imagenet/clean_test_labels_2048.txt'\n",
    "# captions_file = \"../BLIP/experiment_1/cifar10/clean_test_captions.txt\"\n",
    "# test_dataset = CustomDataset(image_folder, label_file, transform=classifier_transform)\n",
    "test_dataset = datasets.CIFAR100(\"../data/\", train=False, transform=classifier_transform, download=True)\n",
    "# train_dataset = datasets.STL10(root=\"../data\",\n",
    "#                            split=\"train\",\n",
    "#                            transform=transform,\n",
    "#                           download=True)\n",
    "# test_dataset = datasets.STL10(root=\"../data\",\n",
    "#                          split=\"test\",\n",
    "#                          transform=transform)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#                                        batch_size=batch_size,\n",
    "#                                        shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                     shuffle=False, num_workers=8)\n",
    "\n",
    "# class_names = train_dataset.classes\n",
    "# data_iterator = iter(train_loader)\n",
    "# batch = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443441fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.plms:\n",
    "    raise NotImplementedError(\"PLMS sampler not (yet) supported\")\n",
    "    sampler = PLMSSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "outpath = opt.outdir\n",
    "\n",
    "batch_size = opt.n_samples\n",
    "n_rows = opt.n_rows if opt.n_rows > 0 else batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce9119ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target t_enc is 25 steps\n"
     ]
    }
   ],
   "source": [
    "# data = [[\"This is a {}\".format(class_names[i]) for i in batch[1]]]\n",
    "# if not opt.from_file:\n",
    "#     prompt = opt.prompt\n",
    "#     assert prompt is not None\n",
    "#     data = [batch_size * [prompt]]\n",
    "# print(data, type(data))\n",
    "# exit()\n",
    "\n",
    "# else:\n",
    "#     print(f\"reading prompts from {opt.from_file}\")\n",
    "#     with open(opt.from_file, \"r\") as f:\n",
    "#         data = f.read().splitlines()\n",
    "#         data = list(chunk(data, batch_size))\n",
    "\n",
    "# sample_path = os.path.join(outpath, \"samples\")\n",
    "# os.makedirs(sample_path, exist_ok=True)\n",
    "# base_count = len(os.listdir(sample_path))\n",
    "# grid_count = len(os.listdir(outpath)) - 1\n",
    "\n",
    "# assert os.path.isfile(opt.init_img)\n",
    "# init_image = load_img(opt.init_img).to(device)\n",
    "# init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "\n",
    "# init_image = batch[0].to(device)\n",
    "# print(init_image.shape, type(init_image))\n",
    "# exit()\n",
    "# init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "# sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(opt.strength * opt.ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "precision_scope = autocast if opt.precision == \"autocast\" else nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58b4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "std = [0.26862954, 0.26130258, 0.27577711]\n",
    "def inverse_normalize(batch_tensor, mean=mean, std=std):\n",
    "    for tensor, m, s in zip(batch_tensor, mean, std):\n",
    "        tensor.mul_(s).add_(m)\n",
    "    return batch_tensor\n",
    "\n",
    "def defense(img):\n",
    "    x_samples = None\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                tic = time.time()\n",
    "                # clear_output(wait=True)\n",
    "                init_image = transform_blip(img).to(device)\n",
    "                # data = [list(batch[2])]\n",
    "                # print(data)\n",
    "                data = [blip_model.generate(init_image, sample=False, num_beams=3, max_length=20, min_length=5)]\n",
    "                # print(data)\n",
    "                num = len(img)\n",
    "                init_image = inverse_normalize(init_image)\n",
    "                init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "                sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "                for n in range(opt.n_iter):\n",
    "                    for prompts in data:\n",
    "                        uc = None\n",
    "                        if opt.scale != 1.0:\n",
    "                            uc = model.get_learned_conditioning(num * [\"\"])\n",
    "                        if isinstance(prompts, tuple):\n",
    "                            prompts = list(prompts)\n",
    "                        c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "                        # encode (scaled latent)\n",
    "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*num).to(device))\n",
    "                        # decode it\n",
    "                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                                unconditional_conditioning=uc,)\n",
    "\n",
    "                        x_samples = model.decode_first_stage(samples)\n",
    "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                toc = time.time()\n",
    "    return inverse_resize(x_samples)\n",
    "# print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "#       f\" \\nEnjoy.\")\n",
    "# if not opt.skip_save:\n",
    "#     for x_sample in x_samples:\n",
    "#         x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "#         Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "#             os.path.join(sample_path, f\"{base_count}.png\"))\n",
    "#         base_count += 1\n",
    "# all_samples.append(x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c6c054",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Standard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# torch.save(model, \"resnet50_stl10_trained_on_diffusion_images.pth\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# classifier = torch.load(\"wrn28_10_cifar10_trained_on_diffusion_images_50.pth\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# classifier = torch.load(\"resnet_50_pretrained_cifar10_trained_on_diffusion_2.pth\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# classifier = load_model(model_name='Standard_R50', dataset='imagenet', threat_model='Linf')\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m classifier \u001b[39m=\u001b[39m load_model(model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mStandard\u001b[39;49m\u001b[39m'\u001b[39;49m, dataset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcifar100\u001b[39;49m\u001b[39m'\u001b[39;49m, threat_model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLinf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m classifier \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/ldmblip/lib/python3.8/site-packages/robustbench/utils.py:135\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name, model_dir, dataset, threat_model, custom_checkpoint, norm)\u001b[0m\n\u001b[1;32m    131\u001b[0m model_path \u001b[39m=\u001b[39m model_dir_ \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    133\u001b[0m models \u001b[39m=\u001b[39m all_models[dataset_][threat_model_]\n\u001b[0;32m--> 135\u001b[0m \u001b[39mif\u001b[39;00m models[model_name][\u001b[39m'\u001b[39m\u001b[39mgdrive_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    137\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel `\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m` nor \u001b[39m\u001b[39m{\u001b[39;00mtimm_model_name\u001b[39m}\u001b[39;00m\u001b[39m aren\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt a timm model and has no `gdrive_id` specified.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(models[model_name][\u001b[39m'\u001b[39m\u001b[39mgdrive_id\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mlist\u001b[39m):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Standard'"
     ]
    }
   ],
   "source": [
    "# torch.save(model, \"resnet50_stl10_trained_on_diffusion_images.pth\")\n",
    "# classifier = torch.load(\"wrn28_10_cifar10_trained_on_diffusion_images_50.pth\")\n",
    "# classifier = torch.load(\"resnet_50_pretrained_cifar10_trained_on_diffusion_2.pth\")\n",
    "# classifier = load_model(model_name='Standard_R50', dataset='imagenet', threat_model='Linf')\n",
    "classifier = load_model(model_name='Standard', dataset='cifar100', threat_model='Linf')\n",
    "classifier = classifier.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1daade",
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = PGD(classifier,eps=0.0, alpha=0.0, defense=defense)\n",
    "atk.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c0c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count = 0\n",
    "for sample in test_loader:\n",
    "    # adv_images = atk(sample[0], sample[1])\n",
    "    # adv_images = classifier_inv_normalize(adv_images)    \n",
    "    generated_image = defense(sample[0])\n",
    "    for a_i in generated_image:\n",
    "        \n",
    "        save_image(a_i, \"../BLIP/experiment_1/imagenet/clean_test_generated_2048/{}.png\".format(count))\n",
    "        # ig_purified = defense(torch.unsqueeze(ig, dim=0))\n",
    "        \n",
    "        # save_image(ig_purified, \"../BLIP/experiment_1/stl_10/adv_test_generated_e2e/cleaned/{}.png\".format(count))\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_purified = defense(torch.unsqueeze(ig, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ig.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "npimg = np.transpose(ig.cpu().numpy(), (1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9514ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defense(nn.Module):\n",
    "    def __init__(self,classifier, args, device, img_size=224):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.classifier = classifier.to(device)\n",
    "\n",
    "        self.image_size = 384\n",
    "        model_path = \"./blip/checkpoints/model_base_capfilt_large.pth\"\n",
    "            \n",
    "        self.blip_model = blip_decoder(pretrained=model_path, image_size=self.image_size, vit='base')\n",
    "        self.blip_model.eval()\n",
    "        self.blip_model = self.blip_model.to(self.device)\n",
    "        seed_everything(self.args.seed)\n",
    "\n",
    "        self.config = OmegaConf.load(f\"{self.args.config}\")\n",
    "        self.model = self.load_model_from_config(self.config, f\"{self.args.ckpt}\")\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.sampler = DDIMSampler(self.model)\n",
    "        self.batch_size = self.args.n_samples\n",
    "        self.t_enc = int(self.args.strength * self.args.ddim_steps)\n",
    "        self.precision_scope = autocast if self.args.precision == \"autocast\" else nullcontext\n",
    "\n",
    "        self.blip_normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        self.transform_blip = transforms.Compose([transforms.Resize(size=(384,384)),\n",
    "                                self.blip_normalize\n",
    "                                ])\n",
    "        self.inverse_resize = transforms.Compose([transforms.Resize((img_size,img_size))])\n",
    "        mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "        std = [0.26862954, 0.26130258, 0.27577711]\n",
    "        self.mean = torch.tensor(mean).view(1, -1, 1, 1).to(self.device)\n",
    "        self.std = torch.tensor(std).view(1, -1, 1, 1).to(self.device)\n",
    "        \n",
    "    def load_model_from_config(self, config, ckpt, verbose=False):\n",
    "        print(f\"Loading model from {ckpt}\")\n",
    "        pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "        if \"global_step\" in pl_sd:\n",
    "            print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "        sd = pl_sd[\"state_dict\"]\n",
    "        model = instantiate_from_config(config.model)\n",
    "        m, u = model.load_state_dict(sd, strict=False)\n",
    "        if len(m) > 0 and verbose:\n",
    "            print(\"missing keys:\")\n",
    "            print(m)\n",
    "        if len(u) > 0 and verbose:\n",
    "            print(\"unexpected keys:\")\n",
    "            print(u)\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        return model\n",
    "    \n",
    "    def inverse_normalize(self, batch_tensor):\n",
    "        unnorm_data = batch_tensor * self.std + self.mean\n",
    "        return unnorm_data\n",
    "\n",
    "    def forward(self, init_image):\n",
    "        with self.precision_scope(\"cuda\"):\n",
    "            with self.model.ema_scope():\n",
    "                init_image = self.transform_blip(init_image).to(self.device)\n",
    "                data = [self.blip_model.generate(init_image, sample=False, num_beams=3, max_length=20, min_length=5)]\n",
    "                num = len(init_image)\n",
    "                init_image = self.inverse_normalize(init_image)\n",
    "                init_latent = self.model.get_first_stage_encoding(self.model.encode_first_stage(init_image))  # move to latent space\n",
    "                self.sampler.make_schedule(ddim_num_steps=self.args.ddim_steps, ddim_eta=self.args.ddim_eta, verbose=False)\n",
    "                # for n in range(self.args.n_iter):\n",
    "                for prompts in data:\n",
    "                    uc = None\n",
    "                    prompts = list(prompts)\n",
    "                    c = self.model.get_learned_conditioning(prompts)\n",
    "\n",
    "                    # encode (scaled latent)\n",
    "                    z_enc = self.sampler.stochastic_encode(init_latent, torch.tensor([self.t_enc]*num).to(self.device))\n",
    "                    # decode it\n",
    "                    init_image = self.sampler.decode(z_enc, c, self.t_enc, unconditional_guidance_scale=self.args.scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "\n",
    "                    init_image = self.model.decode_first_stage(init_image)\n",
    "                    init_image = torch.clamp((init_image + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "        init_image = self.inverse_resize(init_image)\n",
    "        init_image = self.classifier(init_image.to(torch.float16))\n",
    "        return init_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser()\n",
    "model = load_model(model_name='Standard_R50', dataset='imagenet', threat_model='Linf').eval()\n",
    "defense = Defense(model, args, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = args.n_samples\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "imagenet_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "# image_folder = '../BLIP/experiment_1/cifar10/clean_test_generated/'\n",
    "# label_file = '../BLIP/experiment_1/cifar10/clean_test_labels.txt'\n",
    "image_folder = '../BLIP/experiment_1/imagenet/clean_test/'\n",
    "label_file = '../BLIP/experiment_1/imagenet/clean_test_labels.txt'\n",
    "# image_folder = '../BLIP/experiment_1/cifar10/clean_test_generated_1_iter_50_step/'\n",
    "# label_file = '../BLIP/experiment_1/cifar10/clean_test_labels.txt'\n",
    "# # captions_file = \"../BLIP/experiment_1/stl_10/clean_test_captions.txt\"\n",
    "test_dataset = CustomDataset(image_folder, label_file, transform=imagenet_transform)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = []\n",
    "labels_list = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    images_list.append(images)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Concatenate all batches\n",
    "x_test = torch.cat(images_list, 0)\n",
    "y_test = torch.cat(labels_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(torch.float32)\n",
    "classifier(x_test[0].to(torch.float32).unsqueeze(0))\n",
    "x_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14942412",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = clean_accuracy(defense, x_test, y_test, device=device, batch_size=16)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf222b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_attack.autoattack import AutoAttack\n",
    "defense.float().eval()\n",
    "adversary = AutoAttack(defense, norm=\"Linf\", version=\"rand\", eps=4/255, device=device, log_path=\"aa_log.txt\")\n",
    "x_adv = adversary.run_standard_evaluation(x_test[:4], y_test[:4],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpdaattack import BPDAattack\n",
    "def accu(output, target):\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        target = target\n",
    "        assert pred.shape[0] == len(target)\n",
    "        correct = 0\n",
    "        correct += torch.sum(pred == target).item()\n",
    "    return correct / len(target)\n",
    "\n",
    "\n",
    "total_metrics = 0\n",
    "defense_metrics = 0\n",
    "\n",
    "adversary = BPDAattack(classifier.to(torch.float16), defense, device,\n",
    "                                    epsilon=4/255,\n",
    "                                    learning_rate=2/255,\n",
    "                                    max_iterations=40)\n",
    "for batch_idx, (gt_image, label) in enumerate(tqdm(test_loader)):\n",
    "    gt_image, label = gt_image.to(device).to(torch.float16), label.to(device)\n",
    "\n",
    "    adv = adversary.generate(gt_image, label)\n",
    "    adv = adv.detach()\n",
    "\n",
    "    logits = classifier(adv.to(torch.float16))\n",
    "    total_metrics += accu(logits, label)\n",
    "\n",
    "    reformed = defense(adv)\n",
    "    logits = classifier(reformed.to(torch.float16))\n",
    "    defense_metrics += accu(logits, label)\n",
    "\n",
    "total_metrics /= len(test_loader)\n",
    "defense_metrics /= len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_metrics, defense_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98991c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox\n",
    "import torch\n",
    "# from utils import *\n",
    "# from purification import *\n",
    "\n",
    "### Classifer PGD attack\n",
    "# Attack input x\n",
    "def bpda(x, y, network_ebm, network_clf, device=\"cuda\", ball_dim=-1, ptb=8., alpha=2., n_eot=1, iterations=40):\n",
    "    fmodel = foolbox.PyTorchModel(network_clf, bounds=(0., 1.))\n",
    "    x = x.to(device).to(torch.float16)\n",
    "    y = y.to(device)\n",
    "    x_temp = x.clone().detach()\n",
    "    for i in range(iterations):\n",
    "        # get gradient of purified images for n_eot times\n",
    "        grad = torch.zeros_like(x_temp).to(device)\n",
    "        for j in range(n_eot):\n",
    "            x_temp_eot = network_ebm(x_temp)\n",
    "            if ball_dim==-1:\n",
    "                attack = foolbox.attacks.LinfPGD(rel_stepsize=0.05, steps=1, random_start=False) # Can be modified for better attack\n",
    "                _, x_temp_eot_d, _ = attack(fmodel, x_temp_eot, y, epsilons=ptb/255.)\n",
    "            elif ball_dim==2:\n",
    "                attack = foolbox.attacks.L2PGD(rel_stepsize=0.05) # Can be modified for better attack\n",
    "                _, x_temp_eot_d, _ = attack(fmodel, x_temp_eot.float(), y, epsilons=ptb/255.)\n",
    "            grad += (x_temp_eot_d.detach() - x_temp_eot).to(device)\n",
    "        # Check attack success\n",
    "        x_clf = x_temp.clone().detach().to(device)\n",
    "        success = torch.eq(torch.argmax(network_clf(x_clf), dim=1), y)\n",
    "        grad *= success[:, None, None, None] # Attack correctly classified images only\n",
    "        x_temp = torch.clamp(x + torch.clamp(x_temp - x + grad.sign()*alpha/255., -1.0*ptb/255., ptb/255.), 0.0, 1.0)\n",
    "\n",
    "    x_adv = x_temp.clone().detach()\n",
    "    x_clf = x_adv.clone().detach().to(device)\n",
    "    success = torch.eq(torch.argmax(network_clf(network_ebm(x_clf)), dim=1), y)\n",
    "    acc = success.float().mean(axis=-1)\n",
    "\n",
    "    return x_adv, success, acc\n",
    "\n",
    "\n",
    "acc_list = []\n",
    "for batch in tqdm(test_loader):\n",
    "    x,y = batch\n",
    "    _, _, acc = bpda(x,y,defense,classifier.to(torch.float16))\n",
    "    print(acc)\n",
    "    acc_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76731ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([])\n",
    "b = torch.tensor([2.,4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf70eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.cat([a, torch.tensor([acc_list[2].cpu()])])\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d0166be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/128 [00:06<14:34,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.5000)  Current batch: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/128 [00:14<15:20,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.5312)  Current batch: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/128 [00:22<15:43,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4583)  Current batch: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/128 [00:29<14:57,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4844)  Current batch: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/128 [00:36<14:42,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4750)  Current batch: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 6/128 [00:43<14:31,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4688)  Current batch: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 7/128 [00:50<14:14,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4911)  Current batch: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 8/128 [00:57<14:35,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4922)  Current batch: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 9/128 [01:05<14:44,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4861)  Current batch: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10/128 [01:13<14:54,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4625)  Current batch: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 11/128 [01:20<14:23,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4432)  Current batch: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 12/128 [01:27<13:50,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4375)  Current batch: 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 13/128 [01:33<13:16,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4471)  Current batch: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 14/128 [01:40<12:59,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4420)  Current batch: 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 15/128 [01:46<12:38,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4250)  Current batch: 0.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 16/128 [01:53<12:25,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4297)  Current batch: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 17/128 [01:59<12:14,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4412)  Current batch: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 18/128 [02:06<12:02,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4444)  Current batch: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 19/128 [02:12<11:51,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4375)  Current batch: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 20/128 [02:18<11:41,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4313)  Current batch: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 21/128 [02:25<11:36,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4375)  Current batch: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 22/128 [02:31<11:24,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4403)  Current batch: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 23/128 [02:38<11:24,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4375)  Current batch: 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 24/128 [02:45<11:20,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4323)  Current batch: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 25/128 [02:51<11:10,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4300)  Current batch: 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 26/128 [02:57<11:02,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4255)  Current batch: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 27/128 [03:04<10:54,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4213)  Current batch: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 28/128 [03:10<10:45,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4196)  Current batch: 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 29/128 [03:17<10:38,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4246)  Current batch: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 30/128 [03:23<10:33,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4271)  Current batch: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 31/128 [03:30<10:26,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4294)  Current batch: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 32/128 [03:36<10:18,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4238)  Current batch: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 33/128 [03:43<10:17,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: tensor(0.4205)  Current batch: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 33/128 [03:48<10:58,  6.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m x, y  \u001b[39min\u001b[39;00m tqdm(test_loader):\n\u001b[1;32m     23\u001b[0m     x_adv, _, _ \u001b[39m=\u001b[39m clf_pgd(x\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat16),y,defense,classifier\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat16))\n\u001b[0;32m---> 24\u001b[0m     success \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meq(torch\u001b[39m.\u001b[39margmax(classifier(defense(defense(x_adv\u001b[39m.\u001b[39;49mto(device)))), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), y\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     25\u001b[0m     acc \u001b[39m=\u001b[39m success\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     acc_list \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([acc_list, torch\u001b[39m.\u001b[39mtensor([acc\u001b[39m.\u001b[39mitem()])])\n",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36mdefense\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     34\u001b[0m z_enc \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39mstochastic_encode(init_latent, torch\u001b[39m.\u001b[39mtensor([t_enc]\u001b[39m*\u001b[39mnum)\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     35\u001b[0m \u001b[39m# decode it\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m samples \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49mdecode(z_enc, c, t_enc, unconditional_guidance_scale\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mscale,\n\u001b[1;32m     37\u001b[0m                         unconditional_conditioning\u001b[39m=\u001b[39;49muc,)\n\u001b[1;32m     39\u001b[0m x_samples \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode_first_stage(samples)\n\u001b[1;32m     40\u001b[0m x_samples \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp((x_samples \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ldmblip/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/experiments/repos/stable-diffusion/ldm/models/diffusion/ddim.py:253\u001b[0m, in \u001b[0;36mDDIMSampler.decode\u001b[0;34m(self, x_latent, cond, t_start, unconditional_guidance_scale, unconditional_conditioning, use_original_steps)\u001b[0m\n\u001b[1;32m    251\u001b[0m     index \u001b[39m=\u001b[39m total_steps \u001b[39m-\u001b[39m i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    252\u001b[0m     ts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((x_latent\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],), step, device\u001b[39m=\u001b[39mx_latent\u001b[39m.\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m--> 253\u001b[0m     x_dec, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_ddim(x_dec, cond, ts, index\u001b[39m=\u001b[39;49mindex, use_original_steps\u001b[39m=\u001b[39;49muse_original_steps,\n\u001b[1;32m    254\u001b[0m                                   unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[1;32m    255\u001b[0m                                   unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning)\n\u001b[1;32m    256\u001b[0m \u001b[39mreturn\u001b[39;00m x_dec\n",
      "File \u001b[0;32m~/miniconda3/envs/ldmblip/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/experiments/repos/stable-diffusion/ldm/models/diffusion/ddim.py:204\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[0;34m(self, x, c, t, index, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning)\u001b[0m\n\u001b[1;32m    202\u001b[0m sigmas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mddim_sigmas_for_original_num_steps \u001b[39mif\u001b[39;00m use_original_steps \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mddim_sigmas\n\u001b[1;32m    203\u001b[0m \u001b[39m# select parameters corresponding to the currently considered timestep\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m a_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfull((b, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m), alphas[index], device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    205\u001b[0m a_prev \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((b, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), alphas_prev[index], device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    206\u001b[0m sigma_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((b, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), sigmas[index], device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import foolbox\n",
    "import torch\n",
    "\n",
    "### Classifer PGD attack\n",
    "# Attack input x\n",
    "def clf_pgd(x, y, network_ebm, network_clf, device='cuda', ptb=8., dim=-1):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    fmodel = foolbox.PyTorchModel(network_clf, bounds=(0., 1.))\n",
    "    if dim==-1:\n",
    "        attack = foolbox.attacks.LinfPGD(abs_stepsize=2./255., steps=20) # Can be modified for better attack\n",
    "        _, x_adv, success = attack(fmodel, x, y, epsilons=ptb/255.)\n",
    "        acc = 1 - success.float().mean(axis=-1)\n",
    "    elif dim==2:\n",
    "        attack = foolbox.attacks.L2PGD(rel_stepsize=0.25) # Can be modified for better attack\n",
    "        _, x_adv, success = attack(fmodel, x, y, epsilons=ptb/255.)\n",
    "        acc = 1 - success.float().mean(axis=-1)\n",
    "    return x_adv, success, acc\n",
    "\n",
    "\n",
    "acc_list = torch.tensor([])\n",
    "for x, y  in tqdm(test_loader):\n",
    "    x_adv, _, _ = clf_pgd(x.to(torch.float16),y,defense,classifier.to(torch.float16))\n",
    "    success = torch.eq(torch.argmax(classifier(defense(defense(x_adv.to(device)))), dim=1), y.to(device))\n",
    "    acc = success.float().mean(axis=-1)\n",
    "    acc_list = torch.concat([acc_list, torch.tensor([acc.item()])])\n",
    "    print(\"Running:\",torch.mean(acc_list), \" Current batch:\", acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded266da",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = []\n",
    "labels_list = []\n",
    "for images, labels in test_loader:\n",
    "    images_list.append(images)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Concatenate all batches\n",
    "x = torch.cat(images_list, 0)\n",
    "y = torch.cat(labels_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "910d5be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting parameters for standard version\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy: 76.76%\n",
      "apgd-ce - 1/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 2/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 3/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 4/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 5/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 6/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 7/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 8/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 9/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 10/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 11/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 12/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 13/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 14/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 15/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 16/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 17/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 18/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 19/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 20/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 21/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 22/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 23/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 24/25 - 64 out of 64 successfully perturbed\n",
      "apgd-ce - 25/25 - 36 out of 36 successfully perturbed\n",
      "robust accuracy after APGD-CE: 0.00% (total time 219.6 s)\n",
      "max Linf perturbation: 0.01569, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from autoattack import AutoAttack\n",
    "adversary = AutoAttack(classifier, norm='Linf', eps=4/255, version='standard', device=device,)\n",
    "adversary.apgd.n_restarts = 1\n",
    "x_adv = adversary.run_standard_evaluation(x, y, 64,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2261494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "purified = torch.tensor([])\n",
    "for i in x_adv:\n",
    "    i = defense(i.unsqueeze(0)).to(device)\n",
    "    purified = torch.concat([purified, i.detach().cpu()])\n",
    "\n",
    "# acc = clean_accuracy(classifier, x_adv, y, 16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "228166c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = clean_accuracy(classifier, purified, y, 16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2023a498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52734375"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1edb77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = PGD(classifier, eps=4/255, alpha=2/225, steps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c775f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adv = torch.tensor([])\n",
    "for i,j in zip(x,y):\n",
    "    adv = atk(i.unsqueeze(0).to(device), j.unsqueeze(0).to(device))\n",
    "    x_adv = torch.concat([x_adv, adv.detach().cpu()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87122437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 3, 224, 224])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ad74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustbench.model_zoo import cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b404d581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(<ThreatModel.Linf: 'Linf'>,\n",
       "              OrderedDict([('Gowal2020Uncovering',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '16I86x2Vv_HCRKROC86G4dQKgO3Po5mT3'}),\n",
       "                           ('Gowal2020Uncovering_extra',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1LQBdwO2b391mg7VKcP6I0HIOpC6O83gn'}),\n",
       "                           ('Cui2020Learnable_34_20_LBGAT6',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1rN76st8q_32j6Uo8DI5XhcC2cwVhXBwK'}),\n",
       "                           ('Cui2020Learnable_34_10_LBGAT0',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1RnWbGxN-A-ltsfOvulr68U6i2L8ohAJi'}),\n",
       "                           ('Cui2020Learnable_34_10_LBGAT6',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1TfIgvW3BAkL8jL9J7AAWFSLW3SSzJ2AE'}),\n",
       "                           ('Chen2020Efficient',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Chen2020EfficientNet,\n",
       "                             'gdrive_id': '1JEh95fvsfKireoELoVCBxOi12IPGFDUT'}),\n",
       "                           ('Wu2020Adversarial',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Wu2020AdversarialNet,\n",
       "                             'gdrive_id': '1yWGvHmrgjtd9vOpV5zVDqZmeGhCgVYq7'}),\n",
       "                           ('Sehwag2021Proxy',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1ejMNF2O4xkSdrjtZt2UXUeim-y9F7Req'}),\n",
       "                           ('Sitawarin2020Improving',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1hbpwans776KM1SMbOxISkDx0KR0DW8EN'}),\n",
       "                           ('Hendrycks2019Using',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Hendrycks2019UsingNet,\n",
       "                             'gdrive_id': '1If3tppQsCe5dN8Vbo9ff0tjlKQTTrShd'}),\n",
       "                           ('Rice2020Overfitting',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Rice2020OverfittingNet,\n",
       "                             'gdrive_id': '1XXNZn3fZBOkD1aqNL1cvcD8zZDccyAZ6'}),\n",
       "                           ('Rebuffi2021Fixing_70_16_cutmix_ddpm',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-GkVLo9QaRjCJl-by67xda1ySVhYxsLV'}),\n",
       "                           ('Rebuffi2021Fixing_28_10_cutmix_ddpm',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-P7cs82Tj6UVx7Coin3tVurVKYwXWA9p'}),\n",
       "                           ('Rebuffi2021Fixing_R18_ddpm',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-Qcph_EXw1SCYhDIl8cwqTQQy0sJKO8N'}),\n",
       "                           ('Rade2021Helper_R18_ddpm',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-qUvfOjq6x4I8mZynfGtzzCH_nvqS_VQ'}),\n",
       "                           ('Addepalli2021Towards_PARN18',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-FwVya1sDvdFXr0_ZBoXEJW9ukGC7hPK'}),\n",
       "                           ('Addepalli2021Towards_WRN34',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-9GAld_105-jWBLXL73btmfOCwAqvz7Y'}),\n",
       "                           ('Chen2021LTD_WRN34_10',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-I4NZyULdEWH46b4EaCTxuuRo4eFXsg_'}),\n",
       "                           ('Pang2022Robustness_WRN28_10',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1VDDM_j5M4b6sZpt1Nnhkr8FER3kjE33M'}),\n",
       "                           ('Pang2022Robustness_WRN70_16',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1F3kn8KIdBVls8QuTWc3BbB83htkQeVQD'}),\n",
       "                           ('Jia2022LAS-AT_34_10',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-338K2PUf5FTwk4cbUUeTNz247GrXaMG'}),\n",
       "                           ('Jia2022LAS-AT_34_20',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1WhRq01Yl1v8O3skkrGUBuySlptidc5a6'}),\n",
       "                           ('Addepalli2022Efficient_RN18',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-2hnxC7lZOQDqQbum4yPbtRtTND86I5N'}),\n",
       "                           ('Addepalli2022Efficient_WRN_34_10',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-3c-iniqNfiwGoGPHC3nSostnG6J9fDt'}),\n",
       "                           ('Debenedetti2022Light_XCiT-S12',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': None}),\n",
       "                           ('Debenedetti2022Light_XCiT-M12',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': None}),\n",
       "                           ('Debenedetti2022Light_XCiT-L12',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': None}),\n",
       "                           ('Cui2020Learnable_34_10_LBGAT9_eps_8_255',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1TrUAN9opXBQa24WCTVtQTY-RkXK_3dmA'}),\n",
       "                           ('Wang2023Better_WRN-28-10',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-jOu3wKWyw0XnBCXDGR32xn8Bw9GTJgh'}),\n",
       "                           ('Wang2023Better_WRN-70-16',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-yYcT73GP13c0y9HrgtpyB3NAfkGKgjY'}),\n",
       "                           ('Bai2023Improving_edm',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': ['12Z63xr4bQwdIvR2w8xvpUnXG1fnUpy5B',\n",
       "                              '1-yYcT73GP13c0y9HrgtpyB3NAfkGKgjY',\n",
       "                              '1--8Jd82Q9ZLlVS_6c1iPrBxqy9sbdAkx']}),\n",
       "                           ('Bai2023Improving_trades',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': ['12Z63xr4bQwdIvR2w8xvpUnXG1fnUpy5B',\n",
       "                              '1LQBdwO2b391mg7VKcP6I0HIOpC6O83gn',\n",
       "                              '1-0Sffx4gCIydJc16n1H0k6FYE4kHRnGm']}),\n",
       "                           ('Cui2023Decoupled_WRN-28-10',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '15BwWdWbNfoarTEWWugicOVkTjh3nXH36'}),\n",
       "                           ('Cui2023Decoupled_WRN-34-10',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-7GbBqZRaHLFA-kYqcnWl9Q0Ohpicq07'}),\n",
       "                           ('Cui2023Decoupled_WRN-34-10_autoaug',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '18hjcLa1V3JTNUOshafLvw1ncxL2gu50M'})])),\n",
       "             (<ThreatModel.corruptions: 'corruptions'>,\n",
       "              OrderedDict([('Diffenderfer2021Winning_LRR',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Diffenderfer2021CARD,\n",
       "                             'gdrive_id': '1-2egZ5WrO22A2pixw_UxOpENy7zwah8j'}),\n",
       "                           ('Diffenderfer2021Winning_LRR_CARD_Deck',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Diffenderfer2021CARD_Deck,\n",
       "                             'gdrive_id': ['1-9-O8k6FZO0k-WhcIZCXvMBQLutxwF0I',\n",
       "                              '1-H_kInicE70twnsOaK3axVtHBV7WTalI',\n",
       "                              '1-MQjiJy01rc0Wt-dpgEx94pBYIPeXD6F',\n",
       "                              '1-VpIloQl8GePLSYbUjh_Sc0ehZgfiWny',\n",
       "                              '1-i6HADuWHZ8s598mvUL8dIYpL1mxM94f',\n",
       "                              '1-jRg4TpyIYcf-9SeG8vptu4X98VK1ZwE']}),\n",
       "                           ('Diffenderfer2021Winning_Binary',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Diffenderfer2021CARD_Binary,\n",
       "                             'gdrive_id': '1-vFzi6uF6hgORX6sgJt1sKDPcr3SXUxB'}),\n",
       "                           ('Diffenderfer2021Winning_Binary_CARD_Deck',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Diffenderfer2021CARD_Deck_Binary,\n",
       "                             'gdrive_id': ['107TKzt9Nd1ZBx5u-Lc2lgkiqCeiUChw_',\n",
       "                              '10EbQ3BxVQJ0-FyDV42fZL6DEVy5wT7D_',\n",
       "                              '10IRU_otxEVWNRLeG2D4UI5s6O97APCYH',\n",
       "                              '10PyjvWTTyziwpAUxyohkJZZrVHBTwABz',\n",
       "                              '10Skhbub7Uu6_WqQiyzBka4T91-5pOR-K',\n",
       "                              '10_thReUp-ia8Gxq1xdOAFelIHyoMWdV5']}),\n",
       "                           ('Gowal2020Uncovering_Linf',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '16I86x2Vv_HCRKROC86G4dQKgO3Po5mT3'}),\n",
       "                           ('Gowal2020Uncovering_extra_Linf',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1LQBdwO2b391mg7VKcP6I0HIOpC6O83gn'}),\n",
       "                           ('Hendrycks2020AugMix_WRN',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Hendrycks2020AugMixWRNNet,\n",
       "                             'gdrive_id': '1XpFFdCdU9LcDtcyNfo6_BV1RZHKKkBVE'}),\n",
       "                           ('Hendrycks2020AugMix_ResNeXt',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Hendrycks2020AugMixResNeXtNet,\n",
       "                             'gdrive_id': '1ocnHbvDdOBLvgNr6K7vEYL08hUdkD1Rv'}),\n",
       "                           ('Addepalli2021Towards_PARN18',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-FwVya1sDvdFXr0_ZBoXEJW9ukGC7hPK'}),\n",
       "                           ('Addepalli2021Towards_WRN34',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-9GAld_105-jWBLXL73btmfOCwAqvz7Y'}),\n",
       "                           ('Modas2021PRIMEResNet18',\n",
       "                            {'model': robustbench.model_zoo.cifar100.Modas2021PRIMEResNet18,\n",
       "                             'gdrive_id': '1kcohb2tBuJHa5pGSi4nAkvK-hXPSI6Hr'}),\n",
       "                           ('Addepalli2022Efficient_WRN_34_10',\n",
       "                            {'model': <function robustbench.model_zoo.cifar100.<lambda>()>,\n",
       "                             'gdrive_id': '1-3c-iniqNfiwGoGPHC3nSostnG6J9fDt'})]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar100.cifar_100_models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c1786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
